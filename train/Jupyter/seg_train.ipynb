{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-01T14:34:46.182662300Z",
     "start_time": "2024-02-01T14:34:42.043741100Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import csv\n",
    "import logging\n",
    "\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "from train.seg_evaluate import evaluate\n",
    "from models.seg.UNet.unet_model import UNet, R2UNet\n",
    "from utils.data_loading import SegmentDatasetJson, SegmentationDatasetDirectory\n",
    "from models.seg.MaskRCNN.model import MaskRCNNResNet50\n",
    "from utils.indicators import segmentation_indicators\n",
    "from utils.utils import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "image_dir = 'E:/Datas/work/HairEffect/SegmentData/ISIC2018_IMAGES'\n",
    "mask_dir = 'E:/Datas/work/HairEffect/SegmentData/ISIC2018_MASKS_RENAME'\n",
    "\n",
    "val = 0.2\n",
    "size = (256, 256)\n",
    "num_classes = 2\n",
    "epochs = 200\n",
    "batch_size = 8\n",
    "lr = 1e-4\n",
    "amp = True\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T14:34:46.258668400Z",
     "start_time": "2024-02-01T14:34:46.184662900Z"
    }
   },
   "id": "1e2f1e9234229932"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "model = UNet(n_channels=3, n_classes=num_classes, bilinear=False)\n",
    "model = model.to(device=device, memory_format=torch.channels_last)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T14:34:50.406008900Z",
     "start_time": "2024-02-01T14:34:46.260664400Z"
    }
   },
   "id": "a8fe69edab6e24f1"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# set save directory\n",
    "now_time = datetime.now().strftime(\"%Y-%m-%d\") \n",
    "now_h = datetime.now().strftime(\"%H\")\n",
    "\n",
    "dir_checkpoint = Path(f'./checkpoints/UNet')\n",
    "dir_indicators = Path(f'./indicators/UNet')\n",
    "Path(f'{dir_checkpoint}/{now_time}/{now_h}').mkdir(parents=True, exist_ok=True)\n",
    "Path(f'{dir_indicators}/{now_time}/{now_h}').mkdir(parents=True, exist_ok=True)\n",
    "dir_checkpoint_save = Path(f'{dir_checkpoint}/{now_time}/{now_h}')\n",
    "dir_indicators_save = Path(f'{dir_indicators}/{now_time}/{now_h}')\n",
    "\n",
    "# save indicators in csv\n",
    "indicators_header = ['epoch', 'train_loss', 'train_dice', 'train_iou', 'train_f1', 'train_recall',\n",
    "                     'train_precision',\n",
    "                     'val_dice', 'val_iou', 'val_f1', 'val_recall', 'val_precision', 'learning_rate']\n",
    "epoch_csv_path = f\"{dir_indicators_save}/train_indicators.csv\"\n",
    "\n",
    "with open(epoch_csv_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(indicators_header)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T14:34:50.421658300Z",
     "start_time": "2024-02-01T14:34:50.410000300Z"
    }
   },
   "id": "a06397a5bbe0bb90"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Create dataset with 2594\n",
      "INFO:root:Starting training:\n",
      "        Epochs:          200\n",
      "        Batch size:      8\n",
      "        Learning rate:   0.0001\n",
      "        Training size:   2076\n",
      "        Validation size: 518\n",
      "        Device:          cuda\n",
      "        Images size:  (256, 256)\n",
      "        Mixed Precision: True\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "dataset = SegmentationDatasetDirectory(image_dir, mask_dir, size, [0, 255])\n",
    "n_val = int(len(dataset) * val)\n",
    "n_train = len(dataset) - n_val\n",
    "train_set, val_set = random_split(dataset, [n_train, n_val], generator=torch.Generator().manual_seed(0))\n",
    "\n",
    "# Create data loaders os.cpu_count()\n",
    "loader_args = dict(batch_size=batch_size, num_workers=4, pin_memory=True)\n",
    "train_loader = DataLoader(train_set, shuffle=True, **loader_args)\n",
    "val_loader = DataLoader(val_set, shuffle=False, drop_last=True, **loader_args)\n",
    "\n",
    "logging.info(f'''Starting training:\n",
    "        Epochs:          {epochs}\n",
    "        Batch size:      {batch_size}\n",
    "        Learning rate:   {lr}\n",
    "        Training size:   {n_train}\n",
    "        Validation size: {n_val}\n",
    "        Device:          {device.type}\n",
    "        Images size:  {size}\n",
    "        Mixed Precision: {amp}\n",
    "    ''')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T14:34:50.712013800Z",
     "start_time": "2024-02-01T14:34:50.426655500Z"
    }
   },
   "id": "506829150aaf528"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Set up the optimizer, the loss, the learning rate scheduler and the loss scaling for AMP\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=10)  # goal: maximize Dice score\n",
    "grad_scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
    "early_stopping = EarlyStopping(patience=5, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "global_step = 0\n",
    "best_dice = 0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T14:34:50.747911700Z",
     "start_time": "2024-02-01T14:34:50.706075900Z"
    }
   },
   "id": "eed84fb0dbc42a8c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200:   7%|â–‹         | 152/2076 [07:09<1:28:39,  2.77s/img, loss (batch)=0.654]"
     ]
    }
   ],
   "source": [
    "# Begin training\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    \n",
    "    # save epoch train indicators\n",
    "    epoch_train_loss = np.array([])\n",
    "    epoch_train_dice = np.array([])\n",
    "    epoch_train_iou = np.array([])\n",
    "    epoch_train_f1 = np.array([])\n",
    "    epoch_train_recall = np.array([])\n",
    "    epoch_train_precision = np.array([])\n",
    "    # save epoch val\n",
    "    epoch_val_dice = np.array([])\n",
    "    epoch_val_iou = np.array([])\n",
    "    epoch_val_f1 = np.array([])\n",
    "    epoch_val_recall = np.array([])\n",
    "    epoch_val_precision = np.array([])\n",
    "    \n",
    "    # Record the number of verifications performed in each epoch\n",
    "    val_times_every_epoch = 0\n",
    "    \n",
    "    with (tqdm(total=n_train, desc=f'Epoch {epoch}/{epochs}', unit='img') as pbar):\n",
    "        for batch in train_loader:\n",
    "            images, masks = batch['images'], batch['masks']\n",
    "            \n",
    "            assert images.shape[1] == model.n_channels, \\\n",
    "                    f'Network has been defined with {model.n_channels} input channels, ' \\\n",
    "                    f'but loaded images_0 have {images.shape[1]} channels. Please check that ' \\\n",
    "                    'the images_0 are loaded correctly.'\n",
    "            \n",
    "            images_torch = images.to(device=device, dtype=torch.float32, memory_format=torch.channels_last)\n",
    "            masks_torch = masks.to(device=device, dtype=torch.long)\n",
    "            \n",
    "            with torch.autocast(device.type if device.type != 'mps' else 'cpu', enabled=amp):\n",
    "                masks_pred = model(images_torch)\n",
    "                \n",
    "                loss = criterion(masks_pred, masks_torch)\n",
    "                train_indicators_dict = segmentation_indicators(F.softmax(masks_pred, dim=1).float(),\n",
    "                                                                        F.one_hot(masks_torch, num_classes).\n",
    "                                                                        permute(0, 3, 1, 2).float(), multi_class=True, places=3)\n",
    "            d_loss = 1 - train_indicators_dict['dice']\n",
    "            loss += d_loss\n",
    "            \n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            grad_scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            grad_scaler.step(optimizer)\n",
    "            grad_scaler.update()\n",
    "            pbar.update(images.shape[0])\n",
    "            \n",
    "            # update epoch information\n",
    "            epoch_train_loss = np.append(epoch_train_loss, loss.cpu().detach().numpy())\n",
    "            epoch_train_dice = np.append(epoch_train_dice, train_indicators_dict['dice'])\n",
    "            epoch_train_iou = np.append(epoch_train_iou, train_indicators_dict['iou'])\n",
    "            epoch_train_f1 = np.append(epoch_train_f1, train_indicators_dict['f1'])\n",
    "            epoch_train_recall = np.append(epoch_train_recall, train_indicators_dict['recall'])\n",
    "            epoch_train_precision = np.append(epoch_train_precision, train_indicators_dict['precision'])\n",
    "            pbar.set_postfix(**{'loss (batch)': loss.item()})\n",
    "            global_step += 1\n",
    "            \n",
    "            # Evaluation round every 5 batch size do a evaluation\n",
    "            division_step = (n_train // (5 * batch_size))\n",
    "            if division_step > 0:\n",
    "                if global_step % division_step == 0:\n",
    "                    val_times_every_epoch += 1\n",
    "                    val_score = evaluate(model, val_loader, device, amp)\n",
    "                    # score every batch\n",
    "                    val_dice, val_iou, val_f1 = val_score[0], val_score[1], val_score[2]\n",
    "                    val_recall, val_precision = val_score[3], val_score[4]\n",
    "                    \n",
    "                    # average score for every valuate\n",
    "                    val_dice_avg = sum(val_dice) / max(len(val_loader), 1)\n",
    "                    val_iou_avg = sum(val_iou) / max(len(val_loader), 1)\n",
    "                    val_f1_avg = sum(val_f1) / max(len(val_loader), 1)\n",
    "                    val_recall_avg = sum(val_recall) / max(len(val_loader), 1)\n",
    "                    val_precision_avg = sum(val_precision) / max(len(val_loader), 1)\n",
    "                    \n",
    "                    # score for every batch in every epoch\n",
    "                    epoch_val_dice = np.append(epoch_val_dice, val_dice_avg)\n",
    "                    epoch_val_iou = np.append(epoch_val_iou, val_iou_avg)\n",
    "                    epoch_val_f1 = np.append(epoch_val_f1, val_f1_avg)\n",
    "                    epoch_val_recall = np.append(epoch_val_recall, val_recall_avg)\n",
    "                    epoch_val_precision = np.append(epoch_val_precision, val_precision_avg)\n",
    "                    \n",
    "                    logging.info('Validation Dice score: {}'.format(np.round(val_dice_avg, 3)))\n",
    "                    logging.info('Validation Iou score: {}'.format(np.round(val_iou_avg, 3)))\n",
    "                    logging.info('Validation f1 score: {}'.format(np.round(val_f1_avg, 3)))\n",
    "                    logging.info('Validation Recall score: {}'.format(np.round(val_recall_avg, 3)))\n",
    "                    logging.info('Validation Precision score: {}'.format(np.round(val_precision_avg, 3)))\n",
    "                    \n",
    "                    scheduler.step(val_dice_avg)\n",
    "                    \n",
    "    stage = len(train_loader)\n",
    "    train_loss_avg = np.round((sum(epoch_train_loss) / stage), 3)\n",
    "    train_dice_avg = np.round((sum(epoch_train_dice) / stage), 3)\n",
    "    train_iou_avg = np.round((sum(epoch_train_iou) / stage), 3)\n",
    "    train_f1_avg = np.round((sum(epoch_train_f1) / stage), 3)\n",
    "    train_recall_avg = np.round((sum(epoch_train_recall) / stage), 3)\n",
    "    train_precision_avg = np.round((sum(epoch_train_precision) / stage), 3)\n",
    "\n",
    "    epoch_val_dice_avg = np.round((sum(epoch_val_dice) / val_times_every_epoch), 3)\n",
    "    epoch_val_iou_avg = np.round((sum(epoch_val_iou) / val_times_every_epoch), 3)\n",
    "    epoch_val_f1_avg = np.round((sum(epoch_val_f1) / val_times_every_epoch), 3)\n",
    "    epoch_val_recall_avg = np.round((sum(epoch_val_recall) / val_times_every_epoch), 3)\n",
    "    epoch_val_precision_avg = np.round((sum(epoch_val_precision) / val_times_every_epoch), 3)\n",
    "    \n",
    "    stop = early_stopping(epoch_val_dice_avg)\n",
    "    if stop:\n",
    "        break\n",
    "    \n",
    "    epoch_row = [epoch, train_loss_avg, train_dice_avg, train_iou_avg, train_f1_avg,\n",
    "             train_recall_avg, train_precision_avg, epoch_val_dice_avg, epoch_val_iou_avg, epoch_val_f1_avg,\n",
    "             epoch_val_recall_avg, epoch_val_precision_avg, optimizer.param_groups[0]['lr']]\n",
    "    \n",
    "    with open(epoch_csv_path, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(epoch_row)\n",
    "    \n",
    "    state_dict = model.state_dict()\n",
    "    state_dict['mask_values'] = dataset.mask_values\n",
    "    if epoch_val_dice_avg > best_dice:\n",
    "        best_dice = epoch_val_dice_avg\n",
    "        torch.save(state_dict, str(dir_checkpoint_save / f'best.pth'))\n",
    "        logging.info(f'Checkpoint best dice({best_dice}) saved!')\n",
    "    torch.save(state_dict, str(dir_checkpoint_save / f'last.pth'))\n",
    "\n",
    "                    \n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-02-01T14:34:50.741838700Z"
    }
   },
   "id": "7e35252531573c3d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "dc20c169956e3cc1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pytorch",
   "language": "python",
   "display_name": "PyTorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
