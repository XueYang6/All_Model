{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-01T16:03:25.198440600Z",
     "start_time": "2024-02-01T16:03:22.518186600Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import logging\n",
    "import argparse\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, average_precision_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "\n",
    "from train.clf_evaluate import evaluate_model\n",
    "from utils.data_loading import ClassificationDatasetJson\n",
    "from utils.utils import EarlyStopping, stratified_split, ToHSV, ApplyCLAHE\n",
    "from models.clf.ResNet.resnet import ResNetClassification\n",
    "from models.clf.EfficientNet.model import EfficientNetClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "model_name = 'EFFICIENT-NET'\n",
    "num_classes = 2\n",
    "size = (256, 256)\n",
    "epochs = 100\n",
    "batch_size = 8\n",
    "amp = True\n",
    "learning_rate = 1e-4\n",
    "val = 0.2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T16:03:25.214083300Z",
     "start_time": "2024-02-01T16:03:25.198440600Z"
    }
   },
   "id": "37df1cc15ab397b0"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T16:03:25.234855300Z",
     "start_time": "2024-02-01T16:03:25.214083300Z"
    }
   },
   "id": "a61e6a71700aa61"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "MODEL = ['RES-NET', 'EFFICIENT-NET']\n",
    "model = None\n",
    "assert model_name.upper() in MODEL, f'Model optional type are :{MODEL}, please choose available model_name'\n",
    "\n",
    "if model_name.upper() == 'RES-NET':\n",
    "    model = ResNetClassification(num_classes)\n",
    "elif model_name.upper() == 'EFFICIENT-NET':\n",
    "    model = EfficientNetClassification(in_channels=3, num_classes=num_classes)\n",
    "model = model.to(device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T16:03:25.439518800Z",
     "start_time": "2024-02-01T16:03:25.230836100Z"
    }
   },
   "id": "424abe84f9fb28e9"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# set save directory\n",
    "now_time = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "now_h = datetime.now().strftime(\"%H\")\n",
    "\n",
    "dir_checkpoint = Path(f'./MelNv/checkpoints/{model_name}')\n",
    "dir_indicators = Path(f'./MelNv/indicators/{model_name}')\n",
    "Path(f'{dir_checkpoint}/{now_time}/{now_h}').mkdir(parents=True, exist_ok=True)\n",
    "Path(f'{dir_indicators}/{now_time}/{now_h}').mkdir(parents=True, exist_ok=True)\n",
    "dir_checkpoint_save = Path(f'{dir_checkpoint}/{now_time}/{now_h}')\n",
    "dir_indicators_save = Path(f'{dir_indicators}/{now_time}/{now_h}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T16:03:25.486721600Z",
     "start_time": "2024-02-01T16:03:25.439518800Z"
    }
   },
   "id": "da385a08784d42b2"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# set and save header indicators in csv\n",
    "indicators_header = ['epoch', 'train_loss', 'learning_rate','train_acc', 'train_precision', 'train_recall', 'train_f1', 'train_mAP','val_acc', 'val_precision', 'val_recall', 'val_f1', 'val_mAP']\n",
    "epoch_csv_path = f'{dir_indicators_save}/train_indicators.csv'\n",
    "with open(epoch_csv_path, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(indicators_header)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T16:03:25.486721600Z",
     "start_time": "2024-02-01T16:03:25.458089400Z"
    }
   },
   "id": "7cba8630fdef6a05"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(size),\n",
    "    ApplyCLAHE(),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T16:03:25.486721600Z",
     "start_time": "2024-02-01T16:03:25.471092400Z"
    }
   },
   "id": "fe4a608da49cb8e7"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading json file successfully!\n",
      "INFO:root:Start training:\n",
      "Epochs:             100\n",
      "Batch size:         8\n",
      "Learning rate:      0.0001\n",
      "Training size:      8478\n",
      "Validation size:    2119\n",
      "Checkpoints:        True\n",
      "Device:             cuda\n",
      "Images size:        (256, 256)\n",
      "Mixed Precision:    True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create dataset \n",
    "json_data_path = 'E:/Code/Git/All_Model/datasets/json/MelNvTrain_dataset.json'\n",
    "dataset = ClassificationDatasetJson(json_dir=json_data_path, size=size, transform=transform)\n",
    "n_val = int(len(dataset) * val)\n",
    "n_train = len(dataset) - n_val\n",
    "train_set, val_set = random_split(dataset, [n_train, n_val], generator=torch.Generator().manual_seed(0))\n",
    "\n",
    "# create data loaders\n",
    "loader_args = dict(batch_size=batch_size, num_workers=os.cpu_count(), pin_memory=True)\n",
    "train_loader = DataLoader(train_set, shuffle=True, **loader_args)\n",
    "val_loader = DataLoader(val_set, shuffle=False, drop_last=True, **loader_args)\n",
    "\n",
    "# logging\n",
    "logging.info(f'''Start training:\n",
    "Epochs:             {epochs}\n",
    "Batch size:         {batch_size}\n",
    "Learning rate:      {learning_rate}\n",
    "Training size:      {n_train}\n",
    "Validation size:    {n_val}\n",
    "Checkpoints:        {True}\n",
    "Device:             {device.type}\n",
    "Images size:        {size}\n",
    "Mixed Precision:    {amp}\n",
    "''')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T16:03:25.581845500Z",
     "start_time": "2024-02-01T16:03:25.486721600Z"
    }
   },
   "id": "b3d7279d309cc28d"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Set up the optimizer, the loss, the learning rate scheduler and the loss scaling for AMP\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=10)\n",
    "grad_scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
    "early_stopping = EarlyStopping(patience=10, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "best_acc = 0.0\n",
    "global_step = 0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T16:03:25.591713Z",
     "start_time": "2024-02-01T16:03:25.581845500Z"
    }
   },
   "id": "65fe28551eba951e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  11%|█▏        | 968/8478 [08:03<54:48,  2.28image/s, loss (batch)=0.426]  "
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    \n",
    "    val_times_every_epoch = 0 \n",
    "    \n",
    "    # save epoch indicators\n",
    "    epoch_train_loss = 0\n",
    "    epoch_train_acc = 0\n",
    "    epoch_train_precision = 0\n",
    "    epoch_train_recall = 0\n",
    "    epoch_train_f1 = 0\n",
    "    epoch_train_ap = 0\n",
    "\n",
    "    epoch_val_acc = 0\n",
    "    epoch_val_precision = 0\n",
    "    epoch_val_recall = 0\n",
    "    epoch_val_f1 = 0\n",
    "    epoch_val_ap = 0\n",
    "    \n",
    "    with (tqdm(total=n_train, desc=f'Epoch {epoch}/{epochs}', unit='image') as pbar):\n",
    "        for batch in train_loader:\n",
    "            images, true_labels = batch['images'], batch['labels']\n",
    "            images = images.to(device=device, dtype=torch.float32, memory_format=torch.channels_last)\n",
    "            true_labels = true_labels.to(device=device, dtype=torch.long)\n",
    "            \n",
    "            with torch.autocast(device.type if device.type != 'mps' else ' cpu', enabled=amp):\n",
    "                proba_labels = model(images)\n",
    "                pred_labels = torch.argmax(F.softmax(proba_labels, dim=1), dim=1)\n",
    "        \n",
    "                loss = criterion(proba_labels, true_labels)\n",
    "                \n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            grad_scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            grad_scaler.step(optimizer)\n",
    "            grad_scaler.update()\n",
    "            pbar.update(images.shape[0])\n",
    "            \n",
    "            # calculate indicators\n",
    "            true_labels_cpu = true_labels.cpu().numpy()\n",
    "            proba_labels_cpu = proba_labels.detach().cpu().numpy()\n",
    "            pred_labels_cpu = pred_labels.detach().cpu().numpy()\n",
    "            \n",
    "            train_acc = accuracy_score(true_labels_cpu, pred_labels_cpu, normalize=True)\n",
    "            train_precision = precision_score(true_labels_cpu, pred_labels_cpu, average='binary', zero_division=0)\n",
    "            train_recall = recall_score(true_labels_cpu, pred_labels_cpu, zero_division=0)\n",
    "            train_f1 = f1_score(true_labels_cpu, pred_labels_cpu, zero_division=0)\n",
    "            train_ap = average_precision_score(true_labels_cpu, proba_labels_cpu[:, 1])\n",
    "            \n",
    "            # update epoch information\n",
    "            global_step += 1\n",
    "            epoch_train_loss += loss.cpu().detach().numpy()\n",
    "            epoch_train_acc += train_acc\n",
    "            epoch_train_precision += train_precision\n",
    "            epoch_train_recall += train_recall\n",
    "            epoch_train_f1 += train_f1\n",
    "            epoch_train_ap += train_ap\n",
    "            pbar.set_postfix(**{'loss (batch)': loss.item()})\n",
    "            \n",
    "             # Start validation evaluate\n",
    "            division_step = (n_train // (5 * batch_size))  # Evaluation round every 5 batch size do a evaluation\n",
    "            if (global_step % division_step == 0) & (division_step > 0):\n",
    "                val_times_every_epoch += 1\n",
    "                \n",
    "                # indicators in every evaluate\n",
    "                val_acc, val_precision, val_recall, val_f1, val_ap = evaluate_model(model, val_loader, device, amp, num_classes)\n",
    "\n",
    "                # average score for every evaluate\n",
    "                val_acc_avg = sum(val_acc) / max(len(val_loader), 1)\n",
    "                val_precision_avg = sum(val_precision) / max(len(val_loader), 1)\n",
    "                val_recall_avg = sum(val_recall) / max(len(val_loader), 1)\n",
    "                val_f1_avg = sum(val_f1) / max(len(val_loader), 1)\n",
    "                val_ap_avg = sum(val_ap) / max(len(val_loader), 1)\n",
    "\n",
    "                logging.info('Validation ACC score: {}'.format(np.round(val_acc_avg, 3)))\n",
    "                logging.info('Validation Precision score: {}'.format(np.round(val_precision_avg, 3)))\n",
    "                logging.info('Validation Recall score: {}'.format(np.round(val_recall_avg, 3)))\n",
    "                logging.info('Validation F1 score: {}'.format(np.round(val_f1_avg, 3)))\n",
    "                logging.info('Validation AP score: {}'.format(np.round(val_ap_avg, 3)))\n",
    "\n",
    "                # score add in epoch indicators\n",
    "                epoch_val_acc += val_acc_avg\n",
    "                epoch_val_precision += val_precision_avg\n",
    "                epoch_val_recall += val_recall_avg\n",
    "                epoch_val_f1 += val_f1_avg\n",
    "                epoch_val_ap += val_ap_avg\n",
    "\n",
    "                # update scheduler\n",
    "                scheduler.step(val_recall_avg)\n",
    "    \n",
    "    epoch_val_acc_avg = np.round((epoch_val_acc / val_times_every_epoch), 3)\n",
    "    \n",
    "    # set early stop\n",
    "    stop = early_stopping(epoch_val_acc_avg)\n",
    "    if stop:\n",
    "        break\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-02-01T16:03:25.591713Z"
    }
   },
   "id": "d52ec468487ccabd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "84334ffc68507148"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "1f4d8362da63e37e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pytorch",
   "language": "python",
   "display_name": "PyTorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
